# TODO:å®Œæˆæ–°é—»å›¾ç‰‡ä¸é™„ä»¶urlçš„æå–ä¸ä¿å­˜åŠŸèƒ½
# TODO:å¿½ç•¥admission.nju.edu.cnçš„æ–°é—»æ­£æ–‡æå–(å·²å®Œæˆ)
# TODO:passwordå’Œtagæ”¹ä¸ºä»é…ç½®æ–‡ä»¶è¯»å–
# TODO:å¢åŠ æ—¥å¿—è®°å½•åŠŸèƒ½ï¼Œæ›¿ä»£print
# TODO:ç¾åŒ–ä»£ç ç»“æ„ï¼ˆå“­ï¼‰ï¼Œå¢åŠ æ³¨é‡Š
# TODOï¼šæ·»åŠ requerements.txtæ–‡ä»¶

import requests
from bs4 import BeautifulSoup
import re
import json
from urllib.parse import urljoin
import os
from datetime import datetime
import mysql.connector
import hashlib
import configparser


def clean_and_extract_text(soup, selectors):
    """æ ¹æ®é€‰æ‹©å™¨åˆ—è¡¨ï¼Œæ¸…ç†å¹¶æå–æ­£æ–‡æ–‡æœ¬"""
    for selector in selectors:
        content_div = soup.select_one(selector)
        if content_div:
            # æ¸…ç†ä¸å¿…è¦çš„æ ‡ç­¾
            for element in content_div.find_all(
                ['script', 'style', 'iframe', 'div.ctx-music', 'div.control']
            ):
                element.decompose()

            # å°è¯•æå–æ®µè½ï¼Œå¦‚æœå¤±è´¥åˆ™æå–æ‰€æœ‰æ–‡æœ¬
            paragraphs = [
                p.get_text(strip=True)
                for p in content_div.find_all('p')
                if p.get_text(strip=True)
            ]

            if (
                paragraphs and len("".join(paragraphs)) > 50
            ):  # ç¡®ä¿ä¸æ˜¯åªæœ‰å‡ ä¸ªå­—ç¬¦çš„ç©ºå†…å®¹
                return '\n'.join(paragraphs)
            else:
                return content_div.get_text(separator='\n', strip=True)
    return ""


class InfoExtractor:
    """è´Ÿè´£ä»ç‰¹å®š tag é¡µé¢æå–æ–°é—»åˆ—è¡¨å’Œæ­£æ–‡å†…å®¹ï¼Œä¸è´Ÿè´£ä¿å­˜ã€‚"""

    # é›†ä¸­å®šä¹‰é€‰æ‹©å™¨
    WEIXIN_SELECTORS = ['div.rich_media_content']
    NJU_SELECTORS = [
        'div.article-content',
        'div.content',
        '.wp_articlecontent',
        'div#content',
        'div.m-ctx',
        'article',
        'div.post-content',
        'div.entry-content',
    ]

    def __init__(self, tag):
        self.tag = tag
        self.url = f"https://xsxy.nju.edu.cn/sylm/{tag}/index.html"
        self.base_url = "https://xsxy.nju.edu.cn"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
        }

    def get_origin_html(self):
        """è·å–ç›®å½•é¡µ HTML å†…å®¹"""
        try:
            response = requests.get(
                self.url, headers=self.headers, timeout=15
            )  # ç¼©çŸ­è¶…æ—¶æ—¶é—´
            response.raise_for_status()
            print(f"[{self.tag}] æˆåŠŸè·å–ç›®å½•é¡µHTML")
            return response.text
        except requests.exceptions.RequestException as e:
            print(f"[{self.tag}] è¯·æ±‚å¤±è´¥: {e}")
            return None

    def extract_data(self, html_content):
        """ä½¿ç”¨ BeautifulSoup æå– JavaScript ä¸­çš„ dataList"""
        soup = BeautifulSoup(html_content, 'html.parser')
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string and 'dataList' in script.string:
                match = re.search(
                    r'var\s+dataList\s*=\s*(\[.*?\])\s*;', script.string, re.DOTALL
                )
                if match:
                    try:
                        data = json.loads(match.group(1))
                        print(f"[{self.tag}] JSONè§£ææˆåŠŸ")
                        return data
                    except json.JSONDecodeError:
                        print(f"[{self.tag}] JSONè§£æå¤±è´¥")
                        return None
        print(f"[{self.tag}] æœªæ‰¾åˆ°åŒ…å« dataList çš„è„šæœ¬")
        return None

    def normalize_url(self, url):
        """è§„èŒƒåŒ– URLï¼Œå°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„"""
        if not url:
            return ""
        # ä½¿ç”¨ urljoin ç»Ÿä¸€å¤„ç†ç›¸å¯¹è·¯å¾„
        return urljoin(self.base_url, url)

    def classify_url(self, url):
        """æ ¹æ® URL ç‰¹å¾è¿›è¡Œåˆ†ç±»"""
        if not url:
            return 0
        if 'mp.weixin.qq.com' in url:
            return 1  # å¾®ä¿¡å…¬ä¼—å·
        if 'admission.nju.edu.cn' in url:
            return 4  # å¿½ç•¥ç¥ç§˜æ‹›ç”Ÿç½‘é“¾æ¥
        if 'nju.edu.cn' in url:
            return 3  # å—äº¬å¤§å­¦é“¾æ¥
        return 0  # å…¶ä»–

    def get_news_text(self, url):
        """æ ¹æ® URL ç±»å‹ä½¿ç”¨ä¸åŒç­–ç•¥æå–æ–°é—»æ­£æ–‡"""
        category = self.classify_url(url)
        if category == 4:
            return ""  # å¿½ç•¥

        try:
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')

            if category == 1:  # å¾®ä¿¡å…¬ä¼—å·
                return clean_and_extract_text(soup, self.WEIXIN_SELECTORS)
            elif category == 3:  # å—äº¬å¤§å­¦é“¾æ¥
                return clean_and_extract_text(soup, self.NJU_SELECTORS)
            else:  # é€šç”¨/å…¶ä»–
                return clean_and_extract_text(
                    soup, self.NJU_SELECTORS + self.WEIXIN_SELECTORS
                )  # å°è¯•æ‰€æœ‰é€‰æ‹©å™¨

        except Exception as e:
            # print(f"[{self.tag}] âŒ æå–æ­£æ–‡å¤±è´¥ {url}: {e}") # è¿‡äºé¢‘ç¹ï¼Œæ”¹ä¸ºé™é»˜å¤±è´¥
            return ""

    def create_news_item(self, news_dict):
        """ä»å­—å…¸åˆ›å»ºæ–°é—»é¡¹ï¼Œå¹¶æå–æ­£æ–‡å†…å®¹"""
        title = news_dict.get('title', '').strip()
        if not title:
            return None

        url = self.normalize_url(news_dict.get('url', ''))

        news_item = {
            'tag': self.tag,
            'title': title,
            'url': url,
            'date': news_dict.get('daytime', news_dict.get('date', '')),
            'summary': (
                news_dict.get('summary', '')[:100] + '...'
                if news_dict.get('summary')
                else ''
            ),
            'category': self.classify_url(url),
            'content': "",
        }

        if url and url.startswith('http'):
            news_item['content'] = self.get_news_text(url)

        return news_item

    def run(self):
        """è¿è¡Œæå–ç¨‹åºï¼Œè¿”å›æå–åˆ°çš„æ–°é—»åˆ—è¡¨"""
        html = self.get_origin_html()
        if not html:
            return []

        data_list = self.extract_data(html)
        if not data_list:
            return []

        news_items = []
        # ç»Ÿä¸€å¤„ç†æ•°æ®ç»“æ„ï¼šæ— è®ºæ˜¯ä¸€çº§åˆ—è¡¨è¿˜æ˜¯åŒ…å« infolist çš„äºŒçº§ç»“æ„
        raw_list = []
        for item in data_list:
            if (
                isinstance(item, dict)
                and 'infolist' in item
                and isinstance(item['infolist'], list)
            ):
                raw_list.extend(item['infolist'])
            elif isinstance(item, dict) and 'title' in item:
                raw_list.append(item)

        for news in raw_list:
            news_item = self.create_news_item(news)
            if news_item:
                news_items.append(news_item)

        print(f"[{self.tag}]  æå–å¹¶å¤„ç† {len(news_items)} ç¯‡æ–°é—»")
        return news_items


from datetime import datetime
import os
import mysql.connector


class NewsAggregator:
    """è´Ÿè´£åè°ƒæ‰€æœ‰æ ‡ç­¾çš„çˆ¬å–ã€å»é‡å’Œç»Ÿä¸€ä¿å­˜/æ•°æ®åº“æ“ä½œã€‚"""

    def __init__(self, config):
        """åˆå§‹åŒ–é…ç½®å’Œæ•°æ®å­˜å‚¨"""
        # æ•°æ®åº“é…ç½®
        self.db_host = config.get('DATABASE', 'HOST')
        self.db_user = config.get('DATABASE', 'USER')
        self.db_password = config.get('DATABASE', 'PASSWORD')
        self.db_name = config.get('DATABASE', 'DATABASE_NAME')
        self.db_table = 'news_all'  # ç»Ÿä¸€ä¿å­˜åˆ° news_all è¡¨

        # çˆ¬è™«é…ç½®
        tags_str = config.get('CRAWLER', 'TAGS')
        self.tags = [tag.strip() for tag in tags_str.split(',') if tag.strip()]

        self.all_news = []
        self.unique_urls = set()

    def crawl_all_tags(self):
        """éå†æ‰€æœ‰ tagï¼Œæå–æ–°é—»ï¼Œå¹¶è¿›è¡Œå†…å­˜å»é‡"""
        total_extracted = 0
        for tag in self.tags:
            extractor = InfoExtractor(tag=tag)
            news_list = extractor.run()
            total_extracted += len(news_list)

            # å†…å­˜å»é‡ï¼šä½¿ç”¨ URL çš„ MD5 å“ˆå¸Œä½œä¸ºå”¯ä¸€æ ‡è¯†
            for item in news_list:
                url_hash = hashlib.md5(item['url'].encode('utf-8')).hexdigest()

                if url_hash not in self.unique_urls:
                    self.unique_urls.add(url_hash)
                    item['url_hash'] = url_hash
                    self.all_news.append(item)

        print(f"\n==========================================")
        print(
            f"âœ… æ±‡æ€»å®Œæˆï¼æ€»æå– {total_extracted} ç¯‡ï¼Œå»é‡åå¾—åˆ° {len(self.all_news)} ç¯‡å”¯ä¸€æ–°é—»ã€‚"
        )
        print(f"==========================================\n")
        return self.all_news

    def save_to_single_json(self):
        """å°†æ‰€æœ‰æ–°é—»æ•°æ®ä¿å­˜ä¸ºä¸€ä¸ªç»Ÿä¸€çš„ JSON æ–‡ä»¶åˆ°å½“å‰ç›®å½•"""
        if not self.all_news:
            print("âš ï¸ æ²¡æœ‰æ–°é—»æ•°æ®å¯ä¿å­˜åˆ° JSON æ–‡ä»¶ã€‚")
            return 0

        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        filename = f"all_tags_news_{timestamp}.json"

        script_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(script_dir, filename)

        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(self.all_news, f, ensure_ascii=False, indent=2)

            print(f"ğŸ’¾ æˆåŠŸä¿å­˜ {len(self.all_news)} ç¯‡æ–°é—»åˆ°æ±‡æ€»æ–‡ä»¶: {filename}")
            return len(self.all_news)

        except Exception as e:
            print(f"âŒ ä¿å­˜æ±‡æ€»æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
            return 0

    def save_to_database(self):
        """
        å°†æ–°é—»æ•°æ®ä¿å­˜åˆ° MySQL æ•°æ®åº“ï¼ˆä½¿ç”¨ UPSERT é€»è¾‘ï¼‰ã€‚
        å·²ç§»é™¤ tag, created_at, updated_at å­—æ®µçš„å­˜å‚¨ã€‚
        """
        if not self.all_news:
            return

        try:
            connection = mysql.connector.connect(
                host=self.db_host,
                user=self.db_user,
                password=self.db_password,
                database=self.db_name,
            )
            cursor = connection.cursor()

            # --- ã€æ”¹åŠ¨ 1ã€‘ï¼šæ–°çš„ SQL è¯­å¥ï¼Œç§»é™¤ tag, created_at, updated_at ---
            insert_query = f"""
            INSERT INTO {self.db_table} (
                url, title, publish_time, content, 
                image_links, attachment_links, url_hash, crawl_time
            )
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            ON DUPLICATE KEY UPDATE
                title = VALUES(title),
                publish_time = VALUES(publish_time),
                content = VALUES(content),
                crawl_time = VALUES(crawl_time)
            """

            # --- å‡†å¤‡æ•°æ®æ˜ å°„ ---
            data_to_insert = []
            current_crawl_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            for item in self.all_news:
                # é»˜è®¤å€¼å¤„ç†
                image_links = json.dumps([])
                attachment_links = json.dumps([])
                publish_time = item.get('date') if item.get('date') else None

                # --- ã€æ”¹åŠ¨ 2ã€‘ï¼šæ•°æ®å…ƒç»„ï¼Œç§»é™¤ tag å­—æ®µçš„æ•°æ® ---
                data_to_insert.append(
                    (
                        item['url'],  # url
                        item['title'],  # title
                        publish_time,  # publish_time (åŸ date)
                        item['content'],  # content
                        image_links,  # image_links
                        attachment_links,  # attachment_links
                        item['url_hash'],  # url_hash (ç”¨äºå»é‡)
                        current_crawl_time,  # crawl_time (æ–°ç”Ÿæˆ)
                    )
                )

            # --- æ‰§è¡Œæ’å…¥ ---
            cursor.executemany(insert_query, data_to_insert)

            connection.commit()
            print(
                f"ğŸ–¥ï¸ æ•°æ®åº“æ“ä½œå®Œæˆï¼šæˆåŠŸå¤„ç† {len(self.all_news)} æ¡æ–°é—»åˆ° {self.db_table} è¡¨ã€‚"
            )

        except mysql.connector.Error as err:
            print(f"âŒ æ•°æ®åº“é”™è¯¯: {err}")
        finally:
            if 'connection' in locals() and connection.is_connected():
                cursor.close()
                connection.close()

    def run(self):
        """ä¸»æ‰§è¡Œæµç¨‹"""
        self.crawl_all_tags()
        self.save_to_single_json()
        self.save_to_database()


if __name__ == "__main__":

    # 1. åŠ è½½é…ç½®æ–‡ä»¶
    config = configparser.ConfigParser()
    script_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(script_dir, 'config.ini')

    if not os.path.exists(config_path):
        print(f"âŒ é”™è¯¯ï¼šé…ç½®æ–‡ä»¶æœªæ‰¾åˆ°äº {config_path}")
        exit(1)

    config.read(config_path, encoding='utf-8')
    print("âœ… é…ç½®æ–‡ä»¶è¯»å–æˆåŠŸ")

    # 2. å¯åŠ¨èšåˆå™¨
    try:
        aggregator = NewsAggregator(config=config)
        aggregator.run()
    except configparser.NoOptionError as e:
        print(f"âŒ é…ç½®é”™è¯¯ï¼šconfig.ini ä¸­ç¼ºå°‘å¿…è¦çš„é…ç½®é¡¹ï¼š{e}")
    except Exception as e:
        print(f"âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
